{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4031a019",
   "metadata": {},
   "source": [
    "# Plateforme de recommandation des produits Chanel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52c5d034",
   "metadata": {},
   "source": [
    "On commence par importer les modules nécessaires :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0872b507",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import requests\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tqdm import tqdm\n",
    "\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "from keras import *\n",
    "\n",
    "# Configuration\n",
    "IMG_SIZE = (224, 224)  # Taille standard pour ResNet, VGG, etc.\n",
    "CACHE_DIR = \"images_cache\"\n",
    "CHECKPOINT_DIR = \"checkpoints\"\n",
    "BATCH_SIZE = 16\n",
    "SEED = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5ac7915",
   "metadata": {},
   "source": [
    "## Partie 2 : Comparaison des embeddings visuels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceabe8a1",
   "metadata": {},
   "source": [
    "#### Fonction de chargement du checkpoint\n",
    "\n",
    "Cette fonction permet de charger rapidement les données depuis le checkpoint créé dans la partie 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fe95447a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_checkpoint(checkpoint_dir=\"checkpoints\"):\n",
    "    \"\"\"\n",
    "    Charge les données depuis le checkpoint.\n",
    "    Retourne le DataFrame et le LabelEncoder.\n",
    "    \"\"\"\n",
    "    import pickle\n",
    "\n",
    "    data_path = os.path.join(checkpoint_dir, \"data_with_images.csv\")\n",
    "    encoder_path = os.path.join(checkpoint_dir, \"label_encoder.pkl\")\n",
    "\n",
    "    if not os.path.exists(data_path):\n",
    "        raise FileNotFoundError(f\"Checkpoint non trouvé : {data_path}\")\n",
    "\n",
    "    # Charger les données\n",
    "    df = pd.read_csv(data_path)\n",
    "\n",
    "    # Vérifier que les images existent toujours\n",
    "    valid_mask = df[\"image_path\"].apply(\n",
    "        lambda x: os.path.exists(x) if pd.notna(x) else False\n",
    "    )\n",
    "    if not valid_mask.all():\n",
    "        print(f\"Attention: {(~valid_mask).sum()} images manquantes dans le cache\")\n",
    "\n",
    "    # Charger le LabelEncoder\n",
    "    with open(encoder_path, \"rb\") as f:\n",
    "        le = pickle.load(f)\n",
    "\n",
    "    print(f\"Checkpoint chargé : {len(df)} entrées\")\n",
    "    return df, le"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffffcc92",
   "metadata": {},
   "source": [
    "#### Préparation du dataset complet\n",
    "\n",
    "**Pourquoi ne pas faire de split train/validation/test ?**\n",
    "\n",
    "Dans un contexte classique de machine learning, on divise les données en ensembles d'entraînement, validation et test pour :\n",
    "- Évaluer la capacité du modèle à généraliser sur des données non vues\n",
    "- Détecter le sur-apprentissage (overfitting)\n",
    "\n",
    "**Cependant, notre objectif est différent :** nous ne cherchons pas à classifier de nouvelles images, mais à obtenir des **embeddings de qualité** pour notre système de recommandation.\n",
    "\n",
    "Pour un système de recommandation :\n",
    "- Le modèle doit représenter au mieux **l'ensemble du catalogue** existant\n",
    "- Plus le modèle voit de données, meilleures seront les représentations\n",
    "- L'overfitting n'est pas un problème car on veut justement que le modèle \"connaisse\" parfaitement nos produits\n",
    "\n",
    "**Conclusion :** On entraîne sur 100% des données pour obtenir les meilleurs embeddings possibles.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "24eddc6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes avec au moins 2 échantillons : 34 / 39\n",
      "Échantillons utilisables : 895 / 900\n",
      "\n",
      "Dataset complet prêt pour l'entraînement : 895 images\n"
     ]
    }
   ],
   "source": [
    "# Filtrer les classes avec trop peu d'échantillons\n",
    "data_valid = pd.read_csv('checkpoints/data_with_images.csv')\n",
    "num_classes = data_valid[\"label\"].nunique()\n",
    "min_samples_per_class = 2\n",
    "class_counts = data_valid[\"label\"].value_counts()\n",
    "valid_classes = class_counts[class_counts >= min_samples_per_class].index\n",
    "all_images = data_valid[data_valid[\"label\"].isin(valid_classes)].copy()\n",
    "\n",
    "print(\n",
    "    f\"Classes avec au moins {min_samples_per_class} échantillons : {len(valid_classes)} / {num_classes}\"\n",
    ")\n",
    "print(f\"Échantillons utilisables : {len(all_images)} / {len(data_valid)}\")\n",
    "print(f\"\\nDataset complet prêt pour l'entraînement : {len(all_images)} images\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20f229e8",
   "metadata": {},
   "source": [
    "#### 4.5 Chargement des images en arrays NumPy\n",
    "\n",
    "On charge les images depuis le cache et on les convertit en arrays NumPy normalisés (pixels entre 0 et 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "27cfae38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fonction de chargement définie.\n"
     ]
    }
   ],
   "source": [
    "def load_images_to_array(df, desc=\"Chargement\"):\n",
    "    \"\"\"\n",
    "    Charge toutes les images d'un DataFrame en un array NumPy.\n",
    "    Les pixels sont normalisés entre 0 et 1.\n",
    "    \"\"\"\n",
    "    images = []\n",
    "    labels = []\n",
    "\n",
    "    for _, row in tqdm(df.iterrows(), total=len(df), desc=desc):\n",
    "        # Charger l'image\n",
    "        img = Image.open(row[\"image_path\"])\n",
    "        # Convertir en array et normaliser\n",
    "        img_array = np.array(img) / 255.0\n",
    "        images.append(img_array)\n",
    "        labels.append(row[\"label\"])\n",
    "\n",
    "    return np.array(images), np.array(labels)\n",
    "\n",
    "\n",
    "print(\"Fonction de chargement définie.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0353835",
   "metadata": {},
   "source": [
    "On charge les images en arrays NumPy :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "52f185e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠ 2 images manquantes détectées, suppression des lignes correspondantes...\n",
      "  → 893 images restantes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chargement des images: 100%|██████████| 893/893 [00:00<00:00, 1524.99it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Données chargées :\n",
      "  - Total : (893, 224, 224, 3), labels : (893,)\n",
      "  - Nombre de classes : 34\n",
      "\n",
      "Plage de valeurs des pixels : [0.000, 1.000]\n"
     ]
    }
   ],
   "source": [
    "# Vérifier et filtrer les images manquantes avant chargement\n",
    "missing_mask = ~all_images[\"image_path\"].apply(lambda x: os.path.exists(x) if pd.notna(x) else False)\n",
    "missing_count = missing_mask.sum()\n",
    "\n",
    "if missing_count > 0:\n",
    "    print(f\"⚠ {missing_count} images manquantes détectées, suppression des lignes correspondantes...\")\n",
    "    all_images = all_images[~missing_mask].reset_index(drop=True)\n",
    "    print(f\"  → {len(all_images)} images restantes\")\n",
    "\n",
    "# Charger toutes les images en arrays NumPy\n",
    "X_all, y_all = load_images_to_array(all_images, desc=\"Chargement des images\")\n",
    "\n",
    "# Vérification\n",
    "print(f\"\\nDonnées chargées :\")\n",
    "print(f\"  - Total : {X_all.shape}, labels : {y_all.shape}\")\n",
    "print(f\"  - Nombre de classes : {len(np.unique(y_all))}\")\n",
    "print(f\"\\nPlage de valeurs des pixels : [{X_all.min():.3f}, {X_all.max():.3f}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29401498",
   "metadata": {},
   "source": [
    "## Partie 2 : Comparaison des embeddings visuels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca88b672",
   "metadata": {},
   "source": [
    "### 1. Méthodes pour extraire les embeddings visuels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da0013de",
   "metadata": {},
   "source": [
    "#### Méthode 1 : Modèle entraîné pour la classification\n",
    "\n",
    "Entrainement d'un CNN pour classifier les images selon category2_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "da0013de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">220</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">220</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)   │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,432</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">220</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">220</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)   │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">110</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">110</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)   │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">108</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">108</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)   │        <span style=\"color: #00af00; text-decoration-color: #00af00\">18,496</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">108</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">108</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)   │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">54</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">54</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">52</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">52</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │        <span style=\"color: #00af00; text-decoration-color: #00af00\">73,856</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">17</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">17</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">36992</span>)          │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │     <span style=\"color: #00af00; text-decoration-color: #00af00\">9,470,208</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">39</span>)             │        <span style=\"color: #00af00; text-decoration-color: #00af00\">10,023</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv_1 (\u001b[38;5;33mConv2D\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m220\u001b[0m, \u001b[38;5;34m220\u001b[0m, \u001b[38;5;34m32\u001b[0m)   │         \u001b[38;5;34m2,432\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m220\u001b[0m, \u001b[38;5;34m220\u001b[0m, \u001b[38;5;34m32\u001b[0m)   │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d (\u001b[38;5;33mMaxPooling2D\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m110\u001b[0m, \u001b[38;5;34m110\u001b[0m, \u001b[38;5;34m32\u001b[0m)   │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv_2 (\u001b[38;5;33mConv2D\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m108\u001b[0m, \u001b[38;5;34m108\u001b[0m, \u001b[38;5;34m64\u001b[0m)   │        \u001b[38;5;34m18,496\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m108\u001b[0m, \u001b[38;5;34m108\u001b[0m, \u001b[38;5;34m64\u001b[0m)   │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_1 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m54\u001b[0m, \u001b[38;5;34m54\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv_3 (\u001b[38;5;33mConv2D\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m52\u001b[0m, \u001b[38;5;34m52\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │        \u001b[38;5;34m73,856\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_2 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m17\u001b[0m, \u001b[38;5;34m17\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten (\u001b[38;5;33mFlatten\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m36992\u001b[0m)          │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │     \u001b[38;5;34m9,470,208\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_2 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m39\u001b[0m)             │        \u001b[38;5;34m10,023\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">9,575,015</span> (36.53 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m9,575,015\u001b[0m (36.53 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">9,575,015</span> (36.53 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m9,575,015\u001b[0m (36.53 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cnn_model = Sequential(\n",
    "    [\n",
    "        keras.Input(shape=(IMG_SIZE[0], IMG_SIZE[1], 3)),\n",
    "        layers.Conv2D(32, (5, 5), activation=\"relu\", name=\"conv_1\"),\n",
    "        layers.Dropout(0.2),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Conv2D(64, (3, 3), activation=\"relu\", name=\"conv_2\"),\n",
    "        layers.Dropout(0.2),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Conv2D(128, (3, 3), activation=\"relu\", name=\"conv_3\"),\n",
    "        layers.MaxPooling2D((3, 3)),\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(256, activation=\"relu\", name=\"dense_1\"),\n",
    "        layers.Dropout(0.5),\n",
    "        layers.Dense(num_classes, activation=\"softmax\", name=\"dense_2\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "cnn_model.build((None, IMG_SIZE[0], IMG_SIZE[1], 3))\n",
    "\n",
    "cnn_model.summary()\n",
    "\n",
    "reduce_lr = callbacks.ReduceLROnPlateau(\n",
    "    monitor=\"loss\", factor=0.2, patience=5, min_lr=0.000001, verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8e60057f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Modèle CNN chargé depuis : checkpoints/models/cnn_model.keras\n",
      "→ Embeddings manquants, ils seront calculés dans la cellule suivante.\n"
     ]
    }
   ],
   "source": [
    "cnn_model_path = os.path.join(CHECKPOINT_DIR, \"models\", \"cnn_model.keras\")\n",
    "cnn_embeddings_path = os.path.join(CHECKPOINT_DIR, \"embeddings\", \"cnn_embeddings.npy\")\n",
    "os.makedirs(os.path.join(CHECKPOINT_DIR, \"models\"), exist_ok=True)\n",
    "os.makedirs(os.path.join(CHECKPOINT_DIR, \"embeddings\"), exist_ok=True)\n",
    "\n",
    "# Logique: On charge le modèle si disponible, puis on entraîne seulement si nécessaire\n",
    "if os.path.exists(cnn_model_path):\n",
    "    # Charger le modèle existant\n",
    "    cnn_model = models.load_model(cnn_model_path)\n",
    "    print(f\"✓ Modèle CNN chargé depuis : {cnn_model_path}\")\n",
    "    \n",
    "    if os.path.exists(cnn_embeddings_path):\n",
    "        print(f\"✓ Embeddings CNN déjà calculés : {cnn_embeddings_path}\")\n",
    "    else:\n",
    "        print(\"→ Embeddings manquants, ils seront calculés dans la cellule suivante.\")\n",
    "else:\n",
    "    # Le modèle n'existe pas, on doit l'entraîner\n",
    "    print(\"→ Modèle CNN non trouvé, entraînement en cours...\")\n",
    "    \n",
    "    cnn_model.compile(\n",
    "        optimizer=optimizers.Adam(learning_rate=0.001),\n",
    "        loss=\"sparse_categorical_crossentropy\",\n",
    "        metrics=[\"accuracy\"],\n",
    "    )\n",
    "\n",
    "    # Callbacks pour l'entraînement CNN (sans validation)\n",
    "    cnn_checkpoint = callbacks.ModelCheckpoint(\n",
    "        filepath=cnn_model_path,\n",
    "        monitor=\"loss\",\n",
    "        save_best_only=True,\n",
    "        save_weights_only=False,\n",
    "        verbose=1,\n",
    "    )\n",
    "\n",
    "    cnn_early_stopping = callbacks.EarlyStopping(\n",
    "        monitor=\"loss\",\n",
    "        patience=10,\n",
    "        restore_best_weights=True,\n",
    "        verbose=1,\n",
    "        min_delta=0.001,\n",
    "    )\n",
    "\n",
    "    history = cnn_model.fit(\n",
    "        X_all,\n",
    "        y_all,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        epochs=30,\n",
    "        callbacks=[reduce_lr, cnn_checkpoint, cnn_early_stopping],\n",
    "    )\n",
    "\n",
    "    print(f\"✓ Modèle CNN sauvegardé : {cnn_model_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edbeb318",
   "metadata": {},
   "source": [
    "### Création du modèle d'embedding à partir du modèle CNN entrainé\n",
    "\n",
    "Sauvegarde des embeddings des images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bb47ac5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "→ Calcul des embeddings CNN...\n"
     ]
    }
   ],
   "source": [
    "#Calcul des embeddings CNN (uniquement si non existants)\n",
    "if os.path.exists(cnn_embeddings_path):\n",
    "    cnn_embeddings = np.load(cnn_embeddings_path, allow_pickle=True)\n",
    "    print(f\"✓ Embeddings CNN chargés depuis : {cnn_embeddings_path}\")\n",
    "    print(f\"  Shape: {cnn_embeddings.shape}\")\n",
    "    all_images = all_images.reset_index(drop=True)\n",
    "    all_images[\"cnn_embedding\"] = list(cnn_embeddings)\n",
    "else:\n",
    "    print(\"→ Calcul des embeddings CNN...\")\n",
    "    \n",
    "    # Créer le modèle d'embedding (sortie de la couche dense_1)\n",
    "    embedding_model = Model(\n",
    "        inputs=cnn_model.layers[0].input, outputs=cnn_model.get_layer(\"dense_1\").output\n",
    "    )\n",
    "\n",
    "    # Calculer les embeddings sur toutes les images\n",
    "    cnn_embeddings = embedding_model.predict(X_all, batch_size=BATCH_SIZE, verbose=1)\n",
    "\n",
    "    print(f\"  Shape: {cnn_embeddings.shape}\")\n",
    "\n",
    "    # Associer les embeddings au DataFrame\n",
    "    all_images = all_images.reset_index(drop=True)\n",
    "    all_images[\"cnn_embedding\"] = list(cnn_embeddings)\n",
    "\n",
    "    # Sauvegarde des embeddings CNN\n",
    "    np.save(cnn_embeddings_path, cnn_embeddings)\n",
    "    print(f\"✓ Embeddings CNN sauvegardés : {cnn_embeddings_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d15abe3",
   "metadata": {},
   "source": [
    "#### Méthode 2 : Utilisation d’un modèle pré-entraîné"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5d277c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/albanmorin/Desktop/COURS_UQAC/app a/recommendations_projet/recommandations-chanel/venv/lib/python3.12/site-packages/keras/src/layers/layer.py:421: UserWarning: `build()` was called on layer 'pre_trained_classifier', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"pre_trained_classifier\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"pre_trained_classifier\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ convnext_tiny (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)            │    <span style=\"color: #00af00; text-decoration-color: #00af00\">27,820,128</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ convnext_dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)        │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ convnext_dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)        │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ convnext_tiny (\u001b[38;5;33mFunctional\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m768\u001b[0m)            │    \u001b[38;5;34m27,820,128\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ convnext_dense_1 (\u001b[38;5;33mDense\u001b[0m)        │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ convnext_dense_2 (\u001b[38;5;33mDense\u001b[0m)        │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">27,820,128</span> (106.13 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m27,820,128\u001b[0m (106.13 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">27,820,128</span> (106.13 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m27,820,128\u001b[0m (106.13 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Charger ConvNeXt pré-entraîné sur ImageNet\n",
    "convnext_encoder = applications.convnext.ConvNeXtTiny(\n",
    "    weights=\"imagenet\",\n",
    "    include_top=False,\n",
    "    input_shape=(IMG_SIZE[0], IMG_SIZE[1], 3),\n",
    "    pooling=\"avg\",\n",
    ")\n",
    "\n",
    "# Geler les poids du modèle pré-entraîné\n",
    "convnext_encoder.trainable = False\n",
    "\n",
    "\n",
    "@keras.saving.register_keras_serializable()\n",
    "class PreTrainedClassifier(keras.Model):\n",
    "    def __init__(self, encoder=None, num_classes=None, **kwargs):\n",
    "        super(PreTrainedClassifier, self).__init__(**kwargs)\n",
    "        self.num_classes = num_classes\n",
    "        \n",
    "        # L'encoder peut être None lors du chargement (Keras le reconstruira)\n",
    "        if encoder is not None:\n",
    "            self.encoder = encoder\n",
    "        else:\n",
    "            # Recréer l'encoder lors du chargement\n",
    "            self.encoder = applications.convnext.ConvNeXtTiny(\n",
    "                weights=\"imagenet\",\n",
    "                include_top=False,\n",
    "                input_shape=(IMG_SIZE[0], IMG_SIZE[1], 3),\n",
    "                pooling=\"avg\",\n",
    "            )\n",
    "\n",
    "        # Couches Dense pour embeddings et classification\n",
    "        self.dense1 = layers.Dense(256, activation=\"relu\", name=\"convnext_dense_1\")\n",
    "        self.dense2 = layers.Dense(\n",
    "            num_classes, activation=\"softmax\", name=\"convnext_dense_2\"\n",
    "        )\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = self.encoder(inputs)\n",
    "        x = self.dense1(x)\n",
    "        return self.dense2(x)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update(\n",
    "            {\n",
    "                \"num_classes\": self.num_classes,\n",
    "            }\n",
    "        )\n",
    "        return config\n",
    "    \n",
    "    @classmethod\n",
    "    def from_config(cls, config):\n",
    "        return cls(**config)\n",
    "\n",
    "\n",
    "convnext_model = PreTrainedClassifier(convnext_encoder, num_classes=num_classes)\n",
    "convnext_model.build((None, IMG_SIZE[0], IMG_SIZE[1], 3))\n",
    "convnext_model.summary()\n",
    "\n",
    "# Compiler le modèle ConvNeXt\n",
    "convnext_model.compile(\n",
    "    optimizer=optimizers.Adam(learning_rate=0.001),\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\"],\n",
    ")\n",
    "\n",
    "# Prétraitement spécifique à ConvNeXt (normalisation ImageNet)\n",
    "X_all_convnext = applications.convnext.preprocess_input(\n",
    "    X_all * 255\n",
    ")  # Remettre en 0-255 pour le preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a624d7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Modèle ConvNeXt chargé depuis : checkpoints/models/convnext_model.keras\n",
      "✓ Embeddings ConvNeXt déjà calculés : checkpoints/embeddings/convnext_embeddings.npy\n"
     ]
    }
   ],
   "source": [
    "convnext_model_path = os.path.join(CHECKPOINT_DIR, \"models\", \"convnext_model.keras\")\n",
    "convnext_embeddings_path = os.path.join(\n",
    "    CHECKPOINT_DIR, \"embeddings\", \"convnext_embeddings.npy\"\n",
    ")\n",
    "os.makedirs(os.path.join(CHECKPOINT_DIR, \"models\"), exist_ok=True)\n",
    "os.makedirs(os.path.join(CHECKPOINT_DIR, \"embeddings\"), exist_ok=True)\n",
    "\n",
    "# Logique: On charge le modèle si disponible, puis on entraîne seulement si nécessaire\n",
    "if os.path.exists(convnext_model_path):\n",
    "    # Charger le modèle existant avec custom_objects\n",
    "    convnext_model = models.load_model(\n",
    "        convnext_model_path,\n",
    "        custom_objects={\"PreTrainedClassifier\": PreTrainedClassifier}\n",
    "    )\n",
    "    print(f\"✓ Modèle ConvNeXt chargé depuis : {convnext_model_path}\")\n",
    "    \n",
    "    if os.path.exists(convnext_embeddings_path):\n",
    "        print(f\"✓ Embeddings ConvNeXt déjà calculés : {convnext_embeddings_path}\")\n",
    "    else:\n",
    "        print(\"→ Embeddings manquants, ils seront calculés dans la cellule suivante.\")\n",
    "else:\n",
    "    # Le modèle n'existe pas, on doit l'entraîner\n",
    "    print(\"→ Modèle ConvNeXt non trouvé, entraînement en cours...\")\n",
    "    \n",
    "    # Callbacks pour l'entraînement ConvNeXt (phase 1 : head only)\n",
    "    convnext_checkpoint = callbacks.ModelCheckpoint(\n",
    "        filepath=convnext_model_path,\n",
    "        monitor=\"loss\",\n",
    "        save_best_only=True,\n",
    "        save_weights_only=False,\n",
    "        verbose=1,\n",
    "    )\n",
    "\n",
    "    convnext_early_stopping = callbacks.EarlyStopping(\n",
    "        monitor=\"loss\",\n",
    "        patience=5,\n",
    "        restore_best_weights=True,\n",
    "        verbose=1,\n",
    "        min_delta=0.001,\n",
    "    )\n",
    "\n",
    "    # Entraîner le modèle ConvNeXt (phase 1 : head only)\n",
    "    print(\"Phase 1 : Entraînement de la tête de classification...\")\n",
    "    history_convnext_head = convnext_model.fit(\n",
    "        X_all_convnext,\n",
    "        y_all,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        epochs=20,\n",
    "        callbacks=[reduce_lr, convnext_checkpoint, convnext_early_stopping],\n",
    "    )\n",
    "\n",
    "    # Dégeler le modèle ConvNeXt pour un fine-tuning\n",
    "    convnext_encoder.trainable = True\n",
    "\n",
    "    convnext_model.compile(\n",
    "        optimizer=optimizers.Adam(\n",
    "            learning_rate=0.00003\n",
    "        ),  # Learning rate plus bas pour le fine-tuning\n",
    "        loss=\"sparse_categorical_crossentropy\",\n",
    "        metrics=[\"accuracy\"],\n",
    "    )\n",
    "\n",
    "    # Phase 2 : Fine-tuning\n",
    "    print(\"\\nPhase 2 : Fine-tuning du modèle complet...\")\n",
    "    history_convnext = convnext_model.fit(\n",
    "        X_all_convnext,\n",
    "        y_all,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        epochs=5,\n",
    "        callbacks=[reduce_lr, convnext_checkpoint, convnext_early_stopping],\n",
    "    )\n",
    "\n",
    "    print(f\"✓ Modèle ConvNeXt sauvegardé : {convnext_model_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "205d0759",
   "metadata": {},
   "source": [
    "### Extraction des embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0779197",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "→ Calcul des embeddings ConvNeXt...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-03 00:29:35.415633: I external/local_xla/xla/service/service.cc:163] XLA service 0x7d4bc0006160 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2025-12-03 00:29:35.415648: I external/local_xla/xla/service/service.cc:171]   StreamExecutor device (0): Host, Default Version\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1764739775.435861   81550 device_compiler.h:196] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 764ms/step\n",
      "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 764ms/step\n",
      "  Shape: (895, 256)\n",
      "✓ Embeddings ConvNeXt sauvegardés : checkpoints/embeddings/convnext_embeddings.npy\n",
      "  Shape: (895, 256)\n",
      "✓ Embeddings ConvNeXt sauvegardés : checkpoints/embeddings/convnext_embeddings.npy\n"
     ]
    }
   ],
   "source": [
    "# Calcul des embeddings ConvNeXt (uniquement si non existants)\n",
    "if os.path.exists(convnext_embeddings_path):\n",
    "    # Charger les embeddings depuis le fichier\n",
    "    convnext_embeddings = np.load(convnext_embeddings_path)\n",
    "    print(f\"✓ Embeddings ConvNeXt chargés depuis : {convnext_embeddings_path}\")\n",
    "    print(f\"  Shape: {convnext_embeddings.shape}\")\n",
    "    all_images = all_images.reset_index(drop=True)\n",
    "    all_images[\"convnext_embedding\"] = list(convnext_embeddings)\n",
    "else:\n",
    "    print(\"→ Calcul des embeddings ConvNeXt...\")\n",
    "    \n",
    "    X_all_preprocessed = applications.convnext.preprocess_input(\n",
    "        X_all * 255\n",
    "    )  # Remettre en 0-255 pour le preprocessing\n",
    "\n",
    "    # Créer le modèle d'embedding à partir du modèle chargé ou entraîné\n",
    "    # On utilise convnext_model.layers pour accéder aux couches\n",
    "    try:\n",
    "        # Si le modèle a été chargé depuis un fichier\n",
    "        dense1_layer = None\n",
    "        for layer in convnext_model.layers:\n",
    "            if 'dense' in layer.name.lower() and 'convnext_dense_1' in layer.name:\n",
    "                dense1_layer = layer\n",
    "                break\n",
    "        \n",
    "        if dense1_layer is None:\n",
    "            # Fallback: utiliser la structure connue\n",
    "            inputs = convnext_encoder.input\n",
    "            encoder_out = convnext_encoder.output\n",
    "            embeddings_tensor = convnext_model.dense1(encoder_out)\n",
    "            convnext_embedding_model = Model(inputs=inputs, outputs=embeddings_tensor)\n",
    "        else:\n",
    "            inputs = convnext_encoder.input\n",
    "            encoder_out = convnext_encoder.output\n",
    "            embeddings_tensor = dense1_layer(encoder_out)\n",
    "            convnext_embedding_model = Model(inputs=inputs, outputs=embeddings_tensor)\n",
    "    except:\n",
    "        # Structure originale\n",
    "        inputs = convnext_encoder.input\n",
    "        encoder_out = convnext_encoder.output\n",
    "        embeddings_tensor = convnext_model.dense1(encoder_out)\n",
    "        convnext_embedding_model = Model(inputs=inputs, outputs=embeddings_tensor)\n",
    "\n",
    "    # Calculer les embeddings\n",
    "    convnext_embeddings = convnext_embedding_model.predict(\n",
    "        X_all_preprocessed, batch_size=BATCH_SIZE, verbose=1\n",
    "    )\n",
    "\n",
    "    print(f\"  Shape: {convnext_embeddings.shape}\")\n",
    "\n",
    "    # Associer les embeddings au DataFrame\n",
    "    all_images = all_images.reset_index(drop=True)\n",
    "    all_images[\"convnext_embedding\"] = list(convnext_embeddings)\n",
    "\n",
    "    # Sauvegarde des embeddings ConvNeXt\n",
    "    np.save(convnext_embeddings_path, convnext_embeddings)\n",
    "    print(f\"✓ Embeddings ConvNeXt sauvegardés : {convnext_embeddings_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07059b21",
   "metadata": {},
   "source": [
    "#### Fonction utilitaire pour charger les embeddings\n",
    "\n",
    "Cette fonction peut être utilisée pour recharger les embeddings dans une nouvelle session sans avoir à ré-exécuter tout le notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bced823",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fonction load_embeddings_checkpoint() définie (pour utilisation ultérieure)\n"
     ]
    }
   ],
   "source": [
    "def load_embeddings_checkpoint(checkpoint_dir=\"checkpoints\"):\n",
    "    \"\"\"\n",
    "    Charge les embeddings depuis le checkpoint.\n",
    "    Retourne le DataFrame all_images et les arrays numpy des embeddings.\n",
    "    \n",
    "    Note: Cette fonction est utile pour recharger les embeddings dans une nouvelle session\n",
    "    sans avoir à ré-exécuter tout le notebook.\n",
    "    \"\"\"\n",
    "    embeddings_dir = os.path.join(checkpoint_dir, \"embeddings\")\n",
    "\n",
    "    # Charger le DataFrame\n",
    "    df = pd.read_pickle(os.path.join(embeddings_dir, \"all_images_with_embeddings.pkl\"))\n",
    "\n",
    "    # Charger les embeddings numpy (avec vérification)\n",
    "    cnn_path = os.path.join(embeddings_dir, \"cnn_embeddings.npy\")\n",
    "    convnext_path = os.path.join(embeddings_dir, \"convnext_embeddings.npy\")\n",
    "    \n",
    "    cnn_emb = np.load(cnn_path, allow_pickle=True) if os.path.exists(cnn_path) else None\n",
    "    convnext_emb = np.load(convnext_path, allow_pickle=True) if os.path.exists(convnext_path) else None\n",
    "\n",
    "    print(f\"Embeddings chargés : {len(df)} images\")\n",
    "    if cnn_emb is not None and cnn_emb.shape != ():\n",
    "        print(f\"  - CNN: {cnn_emb.shape}\")\n",
    "    else:\n",
    "        print(f\"  - CNN: Non disponible\")\n",
    "    if convnext_emb is not None and convnext_emb.shape != ():\n",
    "        print(f\"  - ConvNeXt: {convnext_emb.shape}\")\n",
    "    else:\n",
    "        print(f\"  - ConvNeXt: Non disponible\")\n",
    "\n",
    "    return df, cnn_emb, convnext_emb\n",
    "\n",
    "\n",
    "# Pour charger depuis le checkpoint dans une nouvelle session (décommenter si besoin) :\n",
    "# all_images, cnn_embeddings, convnext_embeddings = load_embeddings_checkpoint()\n",
    "print(\"Fonction load_embeddings_checkpoint() définie (pour utilisation ultérieure)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e444405",
   "metadata": {},
   "source": [
    "#### Méthode 3 : Self-supervised learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c522d53",
   "metadata": {},
   "source": [
    "On implémente un apprentissage auto-supervisé avec l'approche **SimCLR** (Simple Contrastive Learning of Visual Representations).\n",
    "\n",
    "**Principe de SimCLR :**\n",
    "1. Pour chaque image, on génère deux vues augmentées différentes\n",
    "2. L'encodeur apprend à rapprocher les représentations des deux vues d'une même image (paire positive)\n",
    "3. Et à éloigner les représentations d'images différentes (paires négatives)\n",
    "\n",
    "Cette méthode permet d'apprendre des représentations visuelles sans utiliser les labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "250df4a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modules SimCLR AMÉLIORÉS créés ✓\n",
      "Améliorations:\n",
      "  - Augmentations FORTES (random crop, color jitter, grayscale)\n",
      "  - BatchNormalization dans l'encodeur\n",
      "  - Architecture plus profonde (5 couches conv)\n",
      "  - Option ResNet50 pré-entraîné disponible\n"
     ]
    }
   ],
   "source": [
    "# SimCLR nécessite des augmentations FORTES pour apprendre\n",
    "# des représentations robustes\n",
    "\n",
    "\n",
    "class SimCLRAugmentationSimple(layers.Layer):\n",
    "    \"\"\"\n",
    "    Version simplifiée sans tensorflow_addons.\n",
    "    Utilise des augmentations natives TensorFlow plus fortes.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, img_size=224, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.img_size = img_size\n",
    "\n",
    "    def call(self, images, training=True):\n",
    "        if not training:\n",
    "            return images\n",
    "\n",
    "        batch_size = tf.shape(images)[0]\n",
    "\n",
    "        # 1. Random resized crop (20-100% de l'image) - CRUCIAL!\n",
    "        scale = tf.random.uniform([], 0.3, 1.0)\n",
    "        crop_size = tf.cast(tf.cast(self.img_size, tf.float32) * scale, tf.int32)\n",
    "        crop_size = tf.maximum(crop_size, 48)\n",
    "        images = tf.image.random_crop(images, [batch_size, crop_size, crop_size, 3])\n",
    "        images = tf.image.resize(images, [self.img_size, self.img_size])\n",
    "\n",
    "        # 2. Random flip horizontal\n",
    "        images = tf.image.random_flip_left_right(images)\n",
    "\n",
    "        # 3. Color jitter fort\n",
    "        images = tf.image.random_brightness(images, 0.4)\n",
    "        images = tf.image.random_contrast(images, 0.6, 1.4)\n",
    "        images = tf.image.random_saturation(images, 0.6, 1.4)\n",
    "        images = tf.image.random_hue(images, 0.1)\n",
    "\n",
    "        # 4. Random grayscale (20%)\n",
    "        def to_gray(img):\n",
    "            gray = tf.image.rgb_to_grayscale(img)\n",
    "            return tf.concat([gray, gray, gray], axis=-1)\n",
    "\n",
    "        images = tf.cond(\n",
    "            tf.random.uniform([]) < 0.2, lambda: to_gray(images), lambda: images\n",
    "        )\n",
    "\n",
    "        return tf.clip_by_value(images, 0.0, 1.0)\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# 2. Encodeur AMÉLIORÉ avec BatchNorm et backbone plus profond\n",
    "# ============================================\n",
    "\n",
    "\n",
    "def create_encoder_simple(input_shape=(224, 224, 3), embedding_dim=256):\n",
    "    \"\"\"\n",
    "    Encodeur CNN amélioré avec BatchNormalization.\n",
    "    Architecture plus profonde pour de meilleures représentations.\n",
    "    \"\"\"\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "\n",
    "    # Backbone CNN avec BatchNorm\n",
    "    x = layers.Conv2D(32, (3, 3), padding=\"same\")(inputs)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation(\"relu\")(x)\n",
    "    x = layers.MaxPooling2D((2, 2))(x)\n",
    "\n",
    "    x = layers.Conv2D(64, (3, 3), padding=\"same\")(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation(\"relu\")(x)\n",
    "    x = layers.MaxPooling2D((2, 2))(x)\n",
    "\n",
    "    x = layers.Conv2D(128, (3, 3), padding=\"same\")(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation(\"relu\")(x)\n",
    "    x = layers.MaxPooling2D((2, 2))(x)\n",
    "\n",
    "    x = layers.Conv2D(256, (3, 3), padding=\"same\")(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation(\"relu\")(x)\n",
    "    x = layers.MaxPooling2D((2, 2))(x)\n",
    "\n",
    "    x = layers.Conv2D(512, (3, 3), padding=\"same\")(x)  # Couche supplémentaire\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation(\"relu\")(x)\n",
    "    x = layers.GlobalAveragePooling2D()(x)\n",
    "\n",
    "    # Couche d'embedding avec BN\n",
    "    x = layers.Dense(embedding_dim)(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    embeddings = layers.Activation(\"relu\", name=\"embeddings\")(x)\n",
    "\n",
    "    return Model(inputs, embeddings, name=\"encoder\")\n",
    "\n",
    "\n",
    "def create_encoder_resnet(input_shape=(224, 224, 3), embedding_dim=256):\n",
    "    \"\"\"\n",
    "    Encodeur basé sur ResNet50 pré-entraîné.\n",
    "    Bien meilleur pour des petits datasets!\n",
    "    \"\"\"\n",
    "    # Backbone ResNet50 pré-entraîné\n",
    "    base_model = applications.ResNet50(\n",
    "        include_top=False, weights=\"imagenet\", input_shape=input_shape, pooling=\"avg\"\n",
    "    )\n",
    "\n",
    "    # Congeler les premières couches (optionnel)\n",
    "    for layer in base_model.layers[:-20]:\n",
    "        layer.trainable = False\n",
    "\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "    x = base_model(inputs)\n",
    "    x = layers.Dense(embedding_dim)(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    embeddings = layers.Activation(\"relu\", name=\"embeddings\")(x)\n",
    "\n",
    "    return Model(inputs, embeddings, name=\"encoder_resnet\")\n",
    "\n",
    "\n",
    "def create_projection_head(embedding_dim=256, projection_dim=128):\n",
    "    \"\"\"\n",
    "    Projection head (MLP) amélioré pour SimCLR.\n",
    "    Architecture: embedding_dim -> embedding_dim -> projection_dim\n",
    "    \"\"\"\n",
    "    return tf.keras.Sequential(\n",
    "        [\n",
    "            layers.Dense(embedding_dim),\n",
    "            layers.BatchNormalization(),\n",
    "            layers.Activation(\"relu\"),\n",
    "            layers.Dense(projection_dim),  # Pas d'activation sur la dernière couche\n",
    "        ],\n",
    "        name=\"projection_head\",\n",
    "    )\n",
    "\n",
    "\n",
    "print(\"Modules SimCLR AMÉLIORÉS créés ✓\")\n",
    "print(\"Améliorations:\")\n",
    "print(\"  - Augmentations FORTES (random crop, color jitter, grayscale)\")\n",
    "print(\"  - BatchNormalization dans l'encodeur\")\n",
    "print(\"  - Architecture plus profonde (5 couches conv)\")\n",
    "print(\"  - Option ResNet50 pré-entraîné disponible\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fea6ffc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fonction de perte NT-Xent AMÉLIORÉE définie ✓\n",
      "  - Température réduite à 0.1 (recommandé: 0.07-0.1)\n",
      "  - Memory Bank disponible pour simuler grands batchs\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# 3. Perte contrastive NT-Xent AMÉLIORÉE\n",
    "# ============================================\n",
    "\n",
    "\n",
    "def nt_xent_loss(z_i, z_j, temperature=0.1):\n",
    "    \"\"\"\n",
    "    Calcule la perte NT-Xent (Normalized Temperature-scaled Cross Entropy).\n",
    "\n",
    "    Args:\n",
    "        z_i, z_j: Représentations des deux vues augmentées\n",
    "        temperature: Température pour la softmax (0.07-0.1 recommandé!)\n",
    "\n",
    "    Returns:\n",
    "        Perte contrastive moyenne\n",
    "    \"\"\"\n",
    "    batch_size = tf.shape(z_i)[0]\n",
    "\n",
    "    # Normaliser les vecteurs (L2 norm)\n",
    "    z_i = tf.nn.l2_normalize(z_i, axis=1)\n",
    "    z_j = tf.nn.l2_normalize(z_j, axis=1)\n",
    "\n",
    "    # Concaténer les deux vues: [z_i; z_j] shape: (2B, D)\n",
    "    z = tf.concat([z_i, z_j], axis=0)\n",
    "\n",
    "    # Matrice de similarité cosinus: (2B, 2B)\n",
    "    similarity = tf.matmul(z, z, transpose_b=True) / temperature\n",
    "\n",
    "    # Créer le masque pour exclure l'auto-similarité (diagonale)\n",
    "    batch_size_2 = 2 * batch_size\n",
    "    mask = tf.eye(batch_size_2, dtype=tf.float32)\n",
    "\n",
    "    # Appliquer le masque: -inf sur la diagonale\n",
    "    similarity = similarity - mask * 1e9\n",
    "\n",
    "    # Labels: pour chaque z_i[k], le positif est z_j[k] (à position k+B)\n",
    "    # Pour chaque z_j[k], le positif est z_i[k] (à position k)\n",
    "    labels = tf.concat(\n",
    "        [\n",
    "            tf.range(batch_size, 2 * batch_size),  # z_i -> z_j\n",
    "            tf.range(0, batch_size),  # z_j -> z_i\n",
    "        ],\n",
    "        axis=0,\n",
    "    )\n",
    "\n",
    "    # Cross-entropy loss\n",
    "    loss = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "        labels=labels, logits=similarity\n",
    "    )\n",
    "\n",
    "    return tf.reduce_mean(loss)\n",
    "\n",
    "\n",
    "# Memory Bank pour simuler un grand batch (optionnel mais très utile!)\n",
    "class MemoryBank:\n",
    "    \"\"\"\n",
    "    Memory bank pour stocker les embeddings précédents.\n",
    "    Permet de simuler un grand batch même avec peu de mémoire GPU.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, size=4096, dim=128):\n",
    "        self.size = size\n",
    "        self.dim = dim\n",
    "        self.bank = tf.Variable(\n",
    "            tf.random.normal([size, dim]), trainable=False, name=\"memory_bank\"\n",
    "        )\n",
    "        self.ptr = tf.Variable(0, trainable=False, dtype=tf.int32)\n",
    "\n",
    "    def update(self, embeddings):\n",
    "        \"\"\"Ajoute de nouveaux embeddings au bank\"\"\"\n",
    "        batch_size = tf.shape(embeddings)[0]\n",
    "        embeddings = tf.nn.l2_normalize(embeddings, axis=1)\n",
    "\n",
    "        # Indices où insérer\n",
    "        ptr = self.ptr\n",
    "        end_ptr = (ptr + batch_size) % self.size\n",
    "\n",
    "        # Mise à jour circulaire\n",
    "        if end_ptr > ptr:\n",
    "            self.bank[ptr:end_ptr].assign(embeddings)\n",
    "        else:\n",
    "            remaining = self.size - ptr\n",
    "            self.bank[ptr:].assign(embeddings[:remaining])\n",
    "            self.bank[:end_ptr].assign(embeddings[remaining:])\n",
    "\n",
    "        self.ptr.assign(end_ptr)\n",
    "\n",
    "    def get_negatives(self):\n",
    "        \"\"\"Retourne tous les embeddings stockés\"\"\"\n",
    "        return self.bank\n",
    "\n",
    "\n",
    "print(\"Fonction de perte NT-Xent AMÉLIORÉE définie ✓\")\n",
    "print(\"  - Température réduite à 0.1 (recommandé: 0.07-0.1)\")\n",
    "print(\"  - Memory Bank disponible pour simuler grands batchs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3b2332a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classe SimCLRModel AMÉLIORÉE définie ✓\n",
      "Améliorations:\n",
      "  - Gradient clipping pour stabilité\n",
      "  - Learning rate scheduler avec warmup disponible\n",
      "  - Température par défaut à 0.1\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# 4. Modèle SimCLR complet AMÉLIORÉ\n",
    "# ============================================\n",
    "\n",
    "\n",
    "class SimCLRModel(Model):\n",
    "    \"\"\"\n",
    "    Modèle SimCLR amélioré pour l'apprentissage auto-supervisé.\n",
    "    Avec warmup du learning rate et suivi de la perte.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, encoder, projection_head, augmentation, temperature=0.1, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.encoder = encoder\n",
    "        self.projection_head = projection_head\n",
    "        self.augmentation = augmentation\n",
    "        self.temperature = temperature\n",
    "\n",
    "    def compile(self, optimizer):\n",
    "        super().compile()\n",
    "        self.optimizer = optimizer\n",
    "        self.loss_tracker = tf.keras.metrics.Mean(name=\"loss\")\n",
    "\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        return [self.loss_tracker]\n",
    "\n",
    "    def train_step(self, images):\n",
    "        # Générer deux vues augmentées INDÉPENDANTES\n",
    "        view_1 = self.augmentation(images, training=True)\n",
    "        view_2 = self.augmentation(images, training=True)\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            # Encoder les deux vues\n",
    "            h_1 = self.encoder(view_1, training=True)\n",
    "            h_2 = self.encoder(view_2, training=True)\n",
    "\n",
    "            # Projeter dans l'espace contrastif\n",
    "            z_1 = self.projection_head(h_1, training=True)\n",
    "            z_2 = self.projection_head(h_2, training=True)\n",
    "\n",
    "            # Calculer la perte contrastive\n",
    "            loss = nt_xent_loss(z_1, z_2, self.temperature)\n",
    "\n",
    "        # Gradient et update\n",
    "        trainable_vars = (\n",
    "            self.encoder.trainable_variables + self.projection_head.trainable_variables\n",
    "        )\n",
    "        gradients = tape.gradient(loss, trainable_vars)\n",
    "\n",
    "        # Gradient clipping pour stabilité\n",
    "        gradients, _ = tf.clip_by_global_norm(gradients, 1.0)\n",
    "\n",
    "        self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n",
    "        self.loss_tracker.update_state(loss)\n",
    "\n",
    "        return {\"loss\": self.loss_tracker.result()}\n",
    "\n",
    "    def get_embeddings(self, images):\n",
    "        \"\"\"Retourne les embeddings (sans la projection head).\"\"\"\n",
    "        return self.encoder(images, training=False)\n",
    "    \n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            \"temperature\": self.temperature,\n",
    "        })\n",
    "        return config\n",
    "\n",
    "\n",
    "# Learning rate scheduler avec warmup (très important pour SimCLR!)\n",
    "class WarmUpCosineDecay(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    \"\"\"\n",
    "    Learning rate schedule avec warmup linéaire puis décroissance cosinus.\n",
    "    Crucial pour l'entraînement stable de SimCLR.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, base_lr, total_steps, warmup_steps):\n",
    "        super().__init__()\n",
    "        self.base_lr = base_lr\n",
    "        self.total_steps = total_steps\n",
    "        self.warmup_steps = warmup_steps\n",
    "\n",
    "    def __call__(self, step):\n",
    "        step = tf.cast(step, tf.float32)\n",
    "\n",
    "        # Warmup linéaire\n",
    "        warmup_lr = self.base_lr * (step / self.warmup_steps)\n",
    "\n",
    "        # Cosine decay après warmup\n",
    "        decay_steps = self.total_steps - self.warmup_steps\n",
    "        decay_step = step - self.warmup_steps\n",
    "        cosine_decay = 0.5 * (1 + tf.cos(3.14159 * decay_step / decay_steps))\n",
    "        decay_lr = self.base_lr * cosine_decay\n",
    "\n",
    "        return tf.where(step < self.warmup_steps, warmup_lr, decay_lr)\n",
    "\n",
    "    def get_config(self):\n",
    "        return {\n",
    "            \"base_lr\": self.base_lr,\n",
    "            \"total_steps\": self.total_steps,\n",
    "            \"warmup_steps\": self.warmup_steps,\n",
    "        }\n",
    "\n",
    "\n",
    "print(\"Classe SimCLRModel AMÉLIORÉE définie ✓\")\n",
    "print(\"Améliorations:\")\n",
    "print(\"  - Gradient clipping pour stabilité\")\n",
    "print(\"  - Learning rate scheduler avec warmup disponible\")\n",
    "print(\"  - Température par défaut à 0.1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1181d5a",
   "metadata": {},
   "source": [
    "#### Entraînement du modèle SimCLR\n",
    "\n",
    "On entraîne le modèle SimCLR sur nos images **sans utiliser les labels**. Le modèle apprend à créer des représentations qui regroupent les vues augmentées d'une même image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "585eb364",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Utilisation du CNN personnalisé comme encodeur...\n",
      "\n",
      "==================================================\n",
      "Modèle SimCLR OPTIMISÉ créé:\n",
      "==================================================\n",
      "  • Encodeur: SIMPLE\n",
      "  • Dimension embeddings: 256\n",
      "  • Dimension projection: 128\n",
      "  • Température: 0.1 (←←← IMPORTANTE!)\n",
      "  • Batch size: 64 (plus grand = mieux)\n",
      "  • Epochs: 20\n",
      "  • Warmup: 65 steps (5 epochs)\n",
      "  • Augmentations: FORTES (crop, color jitter, grayscale)\n",
      "==================================================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"encoder\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"encoder\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_11 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)   │           <span style=\"color: #00af00; text-decoration-color: #00af00\">896</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)   │           <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ activation (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)   │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">112</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">112</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)   │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">112</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">112</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)   │        <span style=\"color: #00af00; text-decoration-color: #00af00\">18,496</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_1           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">112</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">112</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)   │           <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ activation_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">112</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">112</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)   │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │        <span style=\"color: #00af00; text-decoration-color: #00af00\">73,856</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_2           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │           <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ activation_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)    │       <span style=\"color: #00af00; text-decoration-color: #00af00\">295,168</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_3           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)    │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ activation_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)    │     <span style=\"color: #00af00; text-decoration-color: #00af00\">1,180,160</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_4           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)    │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,048</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ activation_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_average_pooling2d_2      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling2D</span>)        │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">131,328</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_5           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ embeddings (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_11 (\u001b[38;5;33mInputLayer\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m3\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d (\u001b[38;5;33mConv2D\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m32\u001b[0m)   │           \u001b[38;5;34m896\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m32\u001b[0m)   │           \u001b[38;5;34m128\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ activation (\u001b[38;5;33mActivation\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m32\u001b[0m)   │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_3 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m112\u001b[0m, \u001b[38;5;34m112\u001b[0m, \u001b[38;5;34m32\u001b[0m)   │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_1 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m112\u001b[0m, \u001b[38;5;34m112\u001b[0m, \u001b[38;5;34m64\u001b[0m)   │        \u001b[38;5;34m18,496\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_1           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m112\u001b[0m, \u001b[38;5;34m112\u001b[0m, \u001b[38;5;34m64\u001b[0m)   │           \u001b[38;5;34m256\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ activation_1 (\u001b[38;5;33mActivation\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m112\u001b[0m, \u001b[38;5;34m112\u001b[0m, \u001b[38;5;34m64\u001b[0m)   │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_4 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_2 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │        \u001b[38;5;34m73,856\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_2           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │           \u001b[38;5;34m512\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ activation_2 (\u001b[38;5;33mActivation\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_5 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_3 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m256\u001b[0m)    │       \u001b[38;5;34m295,168\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_3           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m256\u001b[0m)    │         \u001b[38;5;34m1,024\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ activation_3 (\u001b[38;5;33mActivation\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m256\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_6 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m256\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_4 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m512\u001b[0m)    │     \u001b[38;5;34m1,180,160\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_4           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m512\u001b[0m)    │         \u001b[38;5;34m2,048\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ activation_4 (\u001b[38;5;33mActivation\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m512\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_average_pooling2d_2      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "│ (\u001b[38;5;33mGlobalAveragePooling2D\u001b[0m)        │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │       \u001b[38;5;34m131,328\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_5           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │         \u001b[38;5;34m1,024\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ embeddings (\u001b[38;5;33mActivation\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,704,896</span> (6.50 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m1,704,896\u001b[0m (6.50 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,702,400</span> (6.49 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,702,400\u001b[0m (6.49 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,496</span> (9.75 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m2,496\u001b[0m (9.75 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ============================================\n",
    "# Configuration OPTIMISÉE pour SimCLR\n",
    "# ============================================\n",
    "\n",
    "# Paramètres clés - AJUSTÉS pour de meilleures performances\n",
    "SIMCLR_EMBEDDING_DIM = 256\n",
    "SIMCLR_PROJECTION_DIM = 128\n",
    "SIMCLR_TEMPERATURE = 0.1  # Réduit! (était 0.5, recommandé: 0.07-0.1)\n",
    "SIMCLR_EPOCHS = 20  # Augmenté! (était 20)\n",
    "SIMCLR_BATCH_SIZE = 64  # Augmenté! (était 16) - Plus de négatifs par batch\n",
    "\n",
    "# Choix de l'encodeur: 'simple' (CNN léger) ou 'resnet' (ResNet50 pré-entraîné)\n",
    "ENCODER_TYPE = \"simple\"  # Utiliser 'resnet' pour de meilleures performances\n",
    "\n",
    "# Créer l'augmentation FORTE\n",
    "augmentation = SimCLRAugmentationSimple(img_size=IMG_SIZE[0])\n",
    "\n",
    "# Créer l'encodeur selon le choix\n",
    "if ENCODER_TYPE == \"resnet\":\n",
    "    print(\"Utilisation de ResNet50 pré-entraîné comme encodeur...\")\n",
    "    encoder = create_encoder_resnet(\n",
    "        input_shape=(IMG_SIZE[0], IMG_SIZE[1], 3), embedding_dim=SIMCLR_EMBEDDING_DIM\n",
    "    )\n",
    "else:\n",
    "    print(\"Utilisation du CNN personnalisé comme encodeur...\")\n",
    "    encoder = create_encoder_simple(\n",
    "        input_shape=(IMG_SIZE[0], IMG_SIZE[1], 3), embedding_dim=SIMCLR_EMBEDDING_DIM\n",
    "    )\n",
    "\n",
    "# Créer la projection head\n",
    "projection_head = create_projection_head(\n",
    "    embedding_dim=SIMCLR_EMBEDDING_DIM, projection_dim=SIMCLR_PROJECTION_DIM\n",
    ")\n",
    "\n",
    "# Calculer les steps pour le scheduler\n",
    "n_samples = len(all_images)  # Utilise all_images (données filtrées avec images valides)\n",
    "steps_per_epoch = n_samples // SIMCLR_BATCH_SIZE\n",
    "total_steps = steps_per_epoch * SIMCLR_EPOCHS\n",
    "warmup_steps = steps_per_epoch * 5  # 5 epochs de warmup\n",
    "\n",
    "# Learning rate scheduler\n",
    "lr_schedule = WarmUpCosineDecay(\n",
    "    base_lr=0.001, total_steps=total_steps, warmup_steps=warmup_steps  # LR de base\n",
    ")\n",
    "\n",
    "# Créer le modèle SimCLR\n",
    "simclr_model = SimCLRModel(\n",
    "    encoder=encoder,\n",
    "    projection_head=projection_head,\n",
    "    augmentation=augmentation,\n",
    "    temperature=SIMCLR_TEMPERATURE,\n",
    ")\n",
    "\n",
    "# Compiler avec le scheduler\n",
    "simclr_model.compile(optimizer=optimizers.Adam(learning_rate=lr_schedule))\n",
    "\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(f\"Modèle SimCLR OPTIMISÉ créé:\")\n",
    "print(f\"{'='*50}\")\n",
    "print(f\"  • Encodeur: {ENCODER_TYPE.upper()}\")\n",
    "print(f\"  • Dimension embeddings: {SIMCLR_EMBEDDING_DIM}\")\n",
    "print(f\"  • Dimension projection: {SIMCLR_PROJECTION_DIM}\")\n",
    "print(f\"  • Température: {SIMCLR_TEMPERATURE} (←←← IMPORTANTE!)\")\n",
    "print(f\"  • Batch size: {SIMCLR_BATCH_SIZE} (plus grand = mieux)\")\n",
    "print(f\"  • Epochs: {SIMCLR_EPOCHS}\")\n",
    "print(f\"  • Warmup: {warmup_steps} steps ({warmup_steps//steps_per_epoch} epochs)\")\n",
    "print(f\"  • Augmentations: FORTES (crop, color jitter, grayscale)\")\n",
    "print(f\"{'='*50}\\n\")\n",
    "\n",
    "encoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93e1bc84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Encoder SimCLR chargé depuis : checkpoints/models/simclr_encoder.keras\n",
      "→ Embeddings manquants, ils seront calculés dans la cellule suivante.\n"
     ]
    }
   ],
   "source": [
    "# Chemins pour la sauvegarde SimCLR\n",
    "os.makedirs(os.path.join(CHECKPOINT_DIR, \"models\"), exist_ok=True)\n",
    "os.makedirs(os.path.join(CHECKPOINT_DIR, \"embeddings\"), exist_ok=True)\n",
    "simclr_model_path = os.path.join(CHECKPOINT_DIR, \"models\", \"simclr_model.keras\")\n",
    "simclr_encoder_path = os.path.join(CHECKPOINT_DIR, \"models\", \"simclr_encoder.keras\")\n",
    "simclr_embeddings_path = os.path.join(CHECKPOINT_DIR, \"embeddings\", \"simclr_embeddings.npy\")\n",
    "\n",
    "# Logique: On charge l'encoder si disponible, puis on entraîne seulement si nécessaire\n",
    "if os.path.exists(simclr_encoder_path):\n",
    "    # Charger l'encoder existant\n",
    "    encoder = models.load_model(simclr_encoder_path)\n",
    "    print(f\"✓ Encoder SimCLR chargé depuis : {simclr_encoder_path}\")\n",
    "    \n",
    "    if os.path.exists(simclr_embeddings_path):\n",
    "        print(f\"✓ Embeddings SimCLR déjà calculés : {simclr_embeddings_path}\")\n",
    "    else:\n",
    "        print(\"→ Embeddings manquants, ils seront calculés dans la cellule suivante.\")\n",
    "else:\n",
    "    # L'encoder n'existe pas, on doit entraîner SimCLR\n",
    "    print(\"→ Encoder SimCLR non trouvé, entraînement en cours...\")\n",
    "    print(f\"  Données: {len(X_all)} images\")\n",
    "    print(f\"  Batch size: {SIMCLR_BATCH_SIZE} | Epochs: {SIMCLR_EPOCHS}\")\n",
    "    print(f\"  Steps par epoch: {len(X_all) // SIMCLR_BATCH_SIZE}\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "    # Callbacks adaptés\n",
    "    simclr_early_stopping = callbacks.EarlyStopping(\n",
    "        monitor=\"loss\",\n",
    "        patience=10,\n",
    "        restore_best_weights=True,\n",
    "        min_delta=0.001,\n",
    "        verbose=1,\n",
    "    )\n",
    "\n",
    "    simclr_checkpoint = callbacks.ModelCheckpoint(\n",
    "        filepath=simclr_model_path,\n",
    "        monitor=\"loss\",\n",
    "        save_best_only=True,\n",
    "        save_weights_only=False,\n",
    "        verbose=1,\n",
    "    )\n",
    "\n",
    "    class LRLogger(callbacks.Callback):\n",
    "        def on_epoch_end(self, epoch, logs=None):\n",
    "            if hasattr(self.model.optimizer, \"learning_rate\"):\n",
    "                lr = self.model.optimizer.learning_rate\n",
    "                if callable(lr):\n",
    "                    current_lr = float(lr(self.model.optimizer.iterations))\n",
    "                else:\n",
    "                    current_lr = float(lr)\n",
    "                print(f\"  → Learning rate: {current_lr:.6f}\")\n",
    "\n",
    "    history_simclr = simclr_model.fit(\n",
    "        X_all,\n",
    "        epochs=SIMCLR_EPOCHS,\n",
    "        batch_size=SIMCLR_BATCH_SIZE,\n",
    "        verbose=1,\n",
    "        callbacks=[simclr_early_stopping, simclr_checkpoint, LRLogger()],\n",
    "    )\n",
    "\n",
    "    # Visualiser la courbe de perte\n",
    "    plt.figure(figsize=(12, 4))\n",
    "\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history_simclr.history[\"loss\"], \"b-\", linewidth=2)\n",
    "    plt.title(\"Perte contrastive SimCLR (NT-Xent)\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.grid(True, alpha=0.3)\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    loss_diff = np.diff(history_simclr.history[\"loss\"])\n",
    "    plt.plot(loss_diff, \"r-\", linewidth=2)\n",
    "    plt.axhline(y=0, color=\"gray\", linestyle=\"--\")\n",
    "    plt.title(\"Variation de la perte (Δ Loss)\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Δ Loss\")\n",
    "    plt.grid(True, alpha=0.3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    print(f\"\\nPerte finale: {history_simclr.history['loss'][-1]:.4f}\")\n",
    "\n",
    "    # Sauvegarder l'encoder séparément\n",
    "    encoder.save(simclr_encoder_path)\n",
    "    print(f\"✓ Encoder SimCLR sauvegardé : {simclr_encoder_path}\")\n",
    "    print(f\"✓ Modèle SimCLR complet sauvegardé : {simclr_model_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b867d44",
   "metadata": {},
   "source": [
    "#### Extraction des embeddings SimCLR\n",
    "\n",
    "Une fois entraîné, on extrait les embeddings de l'encodeur (sans la projection head) pour les utiliser dans notre système de recommandation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e94e4890",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "→ Calcul des embeddings SimCLR...\n",
      "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 56ms/step\n",
      "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 56ms/step\n",
      "  Shape: (895, 256)\n",
      "✓ Embeddings SimCLR sauvegardés : checkpoints/embeddings/simclr_embeddings.npy\n",
      "  Shape: (895, 256)\n",
      "✓ Embeddings SimCLR sauvegardés : checkpoints/embeddings/simclr_embeddings.npy\n"
     ]
    }
   ],
   "source": [
    "# Calcul des embeddings SimCLR (uniquement si non existants)\n",
    "if os.path.exists(simclr_embeddings_path):\n",
    "    simclr_embeddings = np.load(simclr_embeddings_path)\n",
    "    print(f\"✓ Embeddings SimCLR chargés depuis : {simclr_embeddings_path}\")\n",
    "    print(f\"  Shape: {simclr_embeddings.shape}\")\n",
    "    all_images[\"simclr_embedding\"] = list(simclr_embeddings)\n",
    "else:\n",
    "    print(\"→ Calcul des embeddings SimCLR...\")\n",
    "    \n",
    "    # Utiliser l'encoder (chargé ou entraîné) pour calculer les embeddings\n",
    "    simclr_embeddings = encoder.predict(X_all, batch_size=BATCH_SIZE, verbose=1)\n",
    "\n",
    "    print(f\"  Shape: {simclr_embeddings.shape}\")\n",
    "\n",
    "    # Ajouter au DataFrame\n",
    "    all_images[\"simclr_embedding\"] = list(simclr_embeddings)\n",
    "\n",
    "    # Sauvegarder les embeddings SimCLR\n",
    "    np.save(simclr_embeddings_path, simclr_embeddings)\n",
    "    print(f\"✓ Embeddings SimCLR sauvegardés : {simclr_embeddings_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db31641a",
   "metadata": {},
   "source": [
    "### 2. Comparaison des trois méthodes\n",
    "\n",
    "Analyser la qualité des embeddings sur des critères comme la cohérence intra-classe et les distances inter-classe. \n",
    "* Sélectionner des exemples représentatifs (par exemple, un sac, un parfum, un produit cosmétique) et comparer les distances entre leurs embeddings générés par les trois méthodes. \n",
    "* Visualisation des embeddings (via t-SNE ou UMAP) pour illustrer les regroupements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "281e9b1b",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'cnn_embedding'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/COURS_UQAC/app a/recommendations_projet/recommandations-chanel/venv/lib/python3.12/site-packages/pandas/core/indexes/base.py:3812\u001b[39m, in \u001b[36mIndex.get_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   3811\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3812\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3813\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/index.pyx:167\u001b[39m, in \u001b[36mpandas._libs.index.IndexEngine.get_loc\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/index.pyx:196\u001b[39m, in \u001b[36mpandas._libs.index.IndexEngine.get_loc\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/hashtable_class_helper.pxi:7088\u001b[39m, in \u001b[36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/hashtable_class_helper.pxi:7096\u001b[39m, in \u001b[36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mKeyError\u001b[39m: 'cnn_embedding'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# On suppose que vous avez une colonne 'label' ou 'class' dans all_images\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# et que vos embeddings sont stockés sous forme de liste ou array numpy dans le dataframe\u001b[39;00m\n\u001b[32m      6\u001b[39m \n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# 1. Préparer les matrices (N_images, Dimensions)\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m X_cnn = np.stack(\u001b[43mall_images\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcnn_embedding\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m.values)  \u001b[38;5;66;03m# Shape (N, 256)\u001b[39;00m\n\u001b[32m      9\u001b[39m X_convnext = np.stack(all_images[\u001b[33m\"\u001b[39m\u001b[33mconvnext_embedding\u001b[39m\u001b[33m\"\u001b[39m].values)  \u001b[38;5;66;03m# Shape (N, 256)\u001b[39;00m\n\u001b[32m     10\u001b[39m y_labels = all_images[\u001b[33m\"\u001b[39m\u001b[33mlabel\u001b[39m\u001b[33m\"\u001b[39m].values  \u001b[38;5;66;03m# Vos vraies classes\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/COURS_UQAC/app a/recommendations_projet/recommandations-chanel/venv/lib/python3.12/site-packages/pandas/core/frame.py:4113\u001b[39m, in \u001b[36mDataFrame.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   4111\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.columns.nlevels > \u001b[32m1\u001b[39m:\n\u001b[32m   4112\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._getitem_multilevel(key)\n\u001b[32m-> \u001b[39m\u001b[32m4113\u001b[39m indexer = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4114\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[32m   4115\u001b[39m     indexer = [indexer]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/COURS_UQAC/app a/recommendations_projet/recommandations-chanel/venv/lib/python3.12/site-packages/pandas/core/indexes/base.py:3819\u001b[39m, in \u001b[36mIndex.get_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   3814\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[32m   3815\u001b[39m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc.Iterable)\n\u001b[32m   3816\u001b[39m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[32m   3817\u001b[39m     ):\n\u001b[32m   3818\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[32m-> \u001b[39m\u001b[32m3819\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01merr\u001b[39;00m\n\u001b[32m   3820\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[32m   3821\u001b[39m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[32m   3822\u001b[39m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[32m   3823\u001b[39m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[32m   3824\u001b[39m     \u001b[38;5;28mself\u001b[39m._check_indexing_error(key)\n",
      "\u001b[31mKeyError\u001b[39m: 'cnn_embedding'"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import silhouette_score\n",
    "import numpy as np\n",
    "\n",
    "# On suppose que vous avez une colonne 'label' ou 'class' dans all_images\n",
    "# et que vos embeddings sont stockés sous forme de liste ou array numpy dans le dataframe\n",
    "\n",
    "# 1. Préparer les matrices (N_images, Dimensions)\n",
    "X_cnn = np.stack(all_images[\"cnn_embedding\"].values)  # Shape (N, 256)\n",
    "X_convnext = np.stack(all_images[\"convnext_embedding\"].values)  # Shape (N, 256)\n",
    "y_labels = all_images[\"label\"].values  # Vos vraies classes\n",
    "\n",
    "# 2. Calculer le score (entre -1 et 1. Plus c'est haut, mieux c'est)\n",
    "score_cnn = silhouette_score(X_cnn, y_labels, metric=\"cosine\")\n",
    "score_convnext = silhouette_score(X_convnext, y_labels, metric=\"cosine\")\n",
    "\n",
    "print(\"=== Qualité du clustering (Silhouette Score) ===\")\n",
    "print(f\"  CNN (256d)       : {score_cnn:.4f}\")\n",
    "print(f\"  ConvNeXt (256d)  : {score_convnext:.4f}\")\n",
    "\n",
    "# Vérifier si SimCLR est disponible\n",
    "if \"simclr_embedding\" in all_images.columns:\n",
    "    X_simclr = np.stack(all_images[\"simclr_embedding\"].values)\n",
    "    score_simclr = silhouette_score(X_simclr, y_labels, metric=\"cosine\")\n",
    "    print(f\"  SimCLR (256d)    : {score_simclr:.4f}\")\n",
    "\n",
    "from scipy.spatial.distance import pdist\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "# 1. Calculer toutes les distances par paires pour chaque modèle\n",
    "dists_cnn = pdist(X_cnn, metric=\"cosine\")\n",
    "dists_convnext = pdist(X_convnext, metric=\"cosine\")\n",
    "\n",
    "# 2. Calculer la corrélation de Pearson entre ces distances\n",
    "correlation, _ = pearsonr(dists_cnn, dists_convnext)\n",
    "print(f\"\\nCorrélation entre CNN et ConvNeXt : {correlation:.4f}\")\n",
    "\n",
    "if \"simclr_embedding\" in all_images.columns:\n",
    "    dists_simclr = pdist(X_simclr, metric=\"cosine\")\n",
    "    corr_cnn_simclr, _ = pearsonr(dists_cnn, dists_simclr)\n",
    "    corr_convnext_simclr, _ = pearsonr(dists_convnext, dists_simclr)\n",
    "    print(f\"Corrélation entre CNN et SimCLR    : {corr_cnn_simclr:.4f}\")\n",
    "    print(f\"Corrélation entre ConvNeXt et SimCLR: {corr_convnext_simclr:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "748471d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calcul t-SNE pour CNN...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'X_cnn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      5\u001b[39m tsne = TSNE(n_components=\u001b[32m2\u001b[39m, random_state=\u001b[32m42\u001b[39m, perplexity=\u001b[32m30\u001b[39m)\n\u001b[32m      7\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mCalcul t-SNE pour CNN...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m cnn_2d = tsne.fit_transform(\u001b[43mX_cnn\u001b[49m)\n\u001b[32m     10\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mCalcul t-SNE pour ConvNeXt...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     11\u001b[39m convnext_2d = tsne.fit_transform(X_convnext)\n",
      "\u001b[31mNameError\u001b[39m: name 'X_cnn' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Réduire à 2 dimensions avec t-SNE\n",
    "tsne = TSNE(n_components=2, random_state=42, perplexity=30)\n",
    "\n",
    "print(\"Calcul t-SNE pour CNN...\")\n",
    "cnn_2d = tsne.fit_transform(X_cnn)\n",
    "\n",
    "print(\"Calcul t-SNE pour ConvNeXt...\")\n",
    "convnext_2d = tsne.fit_transform(X_convnext)\n",
    "\n",
    "# Nombre de plots selon si SimCLR est disponible\n",
    "n_plots = 3 if \"simclr_embedding\" in all_images.columns else 2\n",
    "\n",
    "fig, axes = plt.subplots(1, n_plots, figsize=(6 * n_plots, 5))\n",
    "\n",
    "# Plot CNN\n",
    "scatter1 = axes[0].scatter(\n",
    "    cnn_2d[:, 0], cnn_2d[:, 1], c=y_labels, cmap=\"tab20\", s=10, alpha=0.7\n",
    ")\n",
    "axes[0].set_title(f\"CNN (Silhouette: {score_cnn:.3f})\")\n",
    "axes[0].set_xlabel(\"t-SNE 1\")\n",
    "axes[0].set_ylabel(\"t-SNE 2\")\n",
    "\n",
    "# Plot ConvNeXt\n",
    "scatter2 = axes[1].scatter(\n",
    "    convnext_2d[:, 0], convnext_2d[:, 1], c=y_labels, cmap=\"tab20\", s=10, alpha=0.7\n",
    ")\n",
    "axes[1].set_title(f\"ConvNeXt (Silhouette: {score_convnext:.3f})\")\n",
    "axes[1].set_xlabel(\"t-SNE 1\")\n",
    "axes[1].set_ylabel(\"t-SNE 2\")\n",
    "\n",
    "# Plot SimCLR si disponible\n",
    "if \"simclr_embedding\" in all_images.columns:\n",
    "    print(\"Calcul t-SNE pour SimCLR...\")\n",
    "    simclr_2d = tsne.fit_transform(X_simclr)\n",
    "    scatter3 = axes[2].scatter(\n",
    "        simclr_2d[:, 0], simclr_2d[:, 1], c=y_labels, cmap=\"tab20\", s=10, alpha=0.7\n",
    "    )\n",
    "    axes[2].set_title(f\"SimCLR (Silhouette: {score_simclr:.3f})\")\n",
    "    axes[2].set_xlabel(\"t-SNE 1\")\n",
    "    axes[2].set_ylabel(\"t-SNE 2\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n=== Interprétation ===\")\n",
    "print(\"Un score Silhouette plus élevé indique que les embeddings\")\n",
    "print(\"séparent mieux les différentes catégories de produits.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f3bfb48",
   "metadata": {},
   "source": [
    "## Partie 3 : Analyse et comparaison des embeddings textuels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b1c4860",
   "metadata": {},
   "source": [
    "### 1. Traduction des descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe4f2f79",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d0c95548",
   "metadata": {},
   "source": [
    "### 2. Génération des embeddings textuels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dcf22de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "88670129",
   "metadata": {},
   "source": [
    "### 3. Analyse des embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9446f36",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8a870455",
   "metadata": {},
   "source": [
    "### 4. Visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18c2f821",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
